{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6290262",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's import the necessary modules and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7d0a3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Any\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "    \n",
    "# Import our modules\n",
    "from src.gpt_from_scratch.trainer.trainer import ModelTrainer\n",
    "from src.gpt_from_scratch.classic_ngram.classic_ngram import NGramLM, read_lines, build_stream\n",
    "from src.gpt_from_scratch.mini_gpt.mini_gpt import MiniGPT\n",
    "from src.gpt_from_scratch.tokenizer.tokenizer_bpe import BPETokenizer\n",
    "from src.gpt_from_scratch.neural_ngram.neural_ngram import NeuralNGramModel\n",
    "\n",
    "# CHOOSE YOUR DEVICE - pick ONE of these options:\n",
    "\n",
    "# # Option 1: Force CPU (safe but slower)\n",
    "# os.environ[\"DEVICE\"] = \"cpu\"\n",
    "# device = torch.device(\"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# Option 2: Use MPS if available (faster but might have issues)\n",
    "os.environ[\"DEVICE\"] = \"mps\"\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"Using MPS device: {device}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"MPS not available, using: {device}\")\n",
    "\n",
    "# Disable torch compile to avoid MPS compatibility issues\n",
    "torch._dynamo.config.disable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef18d50",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Let's load and prepare the data for our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3837e97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define paths\n",
    "DATA_DIR = project_root / \"data/processed\"\n",
    "TRAIN_PATH = DATA_DIR / \"train.txt\"\n",
    "VAL_PATH = DATA_DIR / \"val.txt\"\n",
    "TOKENIZER_DIR = project_root / \"tokenizers\"\n",
    "\n",
    "# create directories if they don't exist\n",
    "DATA_DIR.mkdir(exist_ok=True, parents=True)\n",
    "TOKENIZER_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# check if data exists, otherwise download/create sample data\n",
    "if not TRAIN_PATH.exists() or not VAL_PATH.exists():\n",
    "    print(\"Data files not found. Creating sample data...\")\n",
    "    \n",
    "    # create simple sample data for demonstration\n",
    "    sample_text = \"\"\"\n",
    "    The quick brown fox jumps over the lazy dog. \n",
    "    To be or not to be, that is the question.\n",
    "    All that glitters is not gold.\n",
    "    Actions speak louder than words.\n",
    "    A picture is worth a thousand words.\n",
    "    \"\"\"\n",
    "    \n",
    "    # split into train/val\n",
    "    lines = [line.strip() for line in sample_text.strip().split('\\n') if line.strip()]\n",
    "    train_lines = lines[:int(len(lines) * 0.8)]\n",
    "    val_lines = lines[int(len(lines) * 0.8):]\n",
    "    \n",
    "    # write to files\n",
    "    DATA_DIR.mkdir(exist_ok=True, parents=True)\n",
    "    with open(TRAIN_PATH, 'w') as f:\n",
    "        f.write('\\n'.join(train_lines))\n",
    "    with open(VAL_PATH, 'w') as f:\n",
    "        f.write('\\n'.join(val_lines))\n",
    "    \n",
    "    print(f\"Created sample data at {TRAIN_PATH} and {VAL_PATH}\")\n",
    "\n",
    "# load the data\n",
    "train_lines = read_lines(TRAIN_PATH)\n",
    "val_lines = read_lines(VAL_PATH)\n",
    "\n",
    "print(f\"train_lines {train_lines}\")\n",
    "\n",
    "print(f\"Loaded {len(train_lines)} training lines and {len(val_lines)} validation lines\")\n",
    "print(\"\\nSample training data:\")\n",
    "for i, line in enumerate(train_lines[:3]):\n",
    "    print(f\"  {i+1}. {line}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54af1fff",
   "metadata": {},
   "source": [
    "## Tokenizer Setup\n",
    "\n",
    "Train a BPE tokenizer on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4580e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training tokenizer with 200 merges ===\n",
      "Trained tokenizer with 200 merges. Vocabulary size: 458\n",
      "\n",
      "=== Training tokenizer with 500 merges ===\n",
      "Trained tokenizer with 500 merges. Vocabulary size: 758\n",
      "\n",
      "=== Training tokenizer with 1000 merges ===\n",
      "Trained tokenizer with 1000 merges. Vocabulary size: 1258\n",
      "\n",
      "Trained 3 tokenizers with merge counts: [200, 500, 1000]\n",
      "\n",
      "Original: The quick brown fox jumps over the lazy dog.\n",
      "Encoded: [0, 297, 417, 107, 378, 295, 116, 425, 322, 113, 122, 34, 108, 119, 111, 114, 263, 113, 366, 298, 325, 124, 269, 102, 113, 105, 48, 1]\n",
      "Decoded: the quick brown fox jumps over the lazy dog.\n",
      "Tokenized: ['', 'the ', 'qu', 'i', 'ck', ' b', 'r', 'own', ' f', 'o', 'x', ' ', 'j', 'u', 'm', 'p', 's ', 'o', 'ver', ' the ', 'la', 'z', 'y ', 'd', 'o', 'g', '.', '']\n"
     ]
    }
   ],
   "source": [
    "def train_tokenizer(merges: int = 200):\n",
    "    \"\"\"Train a BPE tokenizer on the training data.\"\"\"\n",
    "    tokenizer = BPETokenizer(\n",
    "        seed=42,\n",
    "        bos_token=\"<BOS>\",\n",
    "        eos_token=\"<EOS>\",\n",
    "        pad_token=None,\n",
    "        unk_token=None,\n",
    "        lowercase=True,\n",
    "        unicode_normalization=\"NFKC\",\n",
    "        collapse_whitespace=True,\n",
    "        keep_newlines=True,\n",
    "    )\n",
    "    \n",
    "    # train on training data\n",
    "    train_text = TRAIN_PATH.read_text(encoding=\"utf-8\")\n",
    "    tokenizer.train(\n",
    "        train_text,\n",
    "        merges=merges,\n",
    "        guard_train_only=True,\n",
    "        source_tag=\"train\",\n",
    "        min_pair_freq=1,\n",
    "    )\n",
    "    \n",
    "    # save tokenizer\n",
    "    tag = f\"bpe_{merges:04d}\"\n",
    "    tokenizer.save(str(TOKENIZER_DIR), tag)\n",
    "    \n",
    "    print(f\"Trained tokenizer with {merges} merges. Vocabulary size: {len(tokenizer.vocab)}\")\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "#----------more extended training-------\n",
    "# Train the tokenizer\n",
    "#tokenizer = train_tokenizer(merges=200)\n",
    "\n",
    "# for longer experiment\n",
    "#merge_values = [200, 500, 1000]\n",
    "#--------------------------------------\n",
    "\n",
    "# for fastest experiment\n",
    "merge_values = [200]\n",
    "\n",
    "tokenizers = {}\n",
    "\n",
    "for merges in merge_values:\n",
    "    print(f\"\\n=== Training tokenizer with {merges} merges ===\")\n",
    "    tokenizers[merges] = train_tokenizer(merges=merges)\n",
    "    \n",
    "print(f\"\\nTrained {len(tokenizers)} tokenizers with merge counts: {list(tokenizers.keys())}\")\n",
    "\n",
    "tokenizer = tokenizers[200]\n",
    "\n",
    "# test the tokenizer\n",
    "sample_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "encoded = tokenizer.encode(sample_text, add_bos=True, add_eos=True)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(f\"\\nOriginal: {sample_text}\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: {decoded}\")\n",
    "print(f\"Tokenized: {[tokenizer.decode([t]) for t in encoded]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165b740c",
   "metadata": {},
   "source": [
    "## Prepare Data for Models\n",
    "\n",
    "Convert text data to token streams for both model types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1386b451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train stream length: 1999874 tokens\n",
      "Val stream length: 243767 tokens\n",
      "\n",
      "Neural model train input shape: torch.Size([1999810, 64])\n",
      "Neural model val input shape: torch.Size([243703, 64])\n"
     ]
    }
   ],
   "source": [
    "# process data for n-gram models\n",
    "train_stream = build_stream(tokenizer, train_lines)\n",
    "val_stream = build_stream(tokenizer, val_lines)\n",
    "\n",
    "print(f\"Train stream length: {len(train_stream)} tokens\")\n",
    "print(f\"Val stream length: {len(val_stream)} tokens\")\n",
    "\n",
    "# create tensor datasets for neural models\n",
    "def create_tensor_dataset(token_stream, context_size=64):\n",
    "    data = torch.tensor(token_stream, dtype=torch.long)\n",
    "    \n",
    "    # create sequences of context_size + 1\n",
    "    sequences = []\n",
    "    for i in range(0, len(data) - context_size):\n",
    "        seq = data[i:i+context_size+1]\n",
    "        sequences.append(seq)\n",
    "    \n",
    "    if sequences:\n",
    "        # stack sequences into a tensor\n",
    "        tensor_data = torch.stack(sequences)\n",
    "        \n",
    "        # split into inputs and targets\n",
    "        x = tensor_data[:, :-1]  # all but last token\n",
    "        y = tensor_data[:, 1:]   # all but first token\n",
    "        \n",
    "        return x, y\n",
    "    else:\n",
    "        # handle case with too short data\n",
    "        print(\"Warning: Data too short to create sequences!\")\n",
    "        return torch.tensor([], dtype=torch.long), torch.tensor([], dtype=torch.long)\n",
    "\n",
    "# create tensor data for neural models\n",
    "context_size = 64  # Context window size\n",
    "x_train, y_train = create_tensor_dataset(train_stream, context_size)\n",
    "x_val, y_val = create_tensor_dataset(val_stream, context_size)\n",
    "\n",
    "print(f\"\\nNeural model train input shape: {x_train.shape if len(x_train) > 0 else 'Empty'}\")\n",
    "print(f\"Neural model val input shape: {x_val.shape if len(x_val) > 0 else 'Empty'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f092ea",
   "metadata": {},
   "source": [
    "## Training Configuration\n",
    "\n",
    "Define configurations for both model types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b636b9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------For faster experiment----------------\n",
    "\n",
    "ngram_configs = [\n",
    "    {\"n\": 1, \"delta\": 0.1, \"smoothing\": \"laplace\", \"epochs\": 1, \"learning_rate\": 0.01, \"batch_size\": 32, \"context_size\": context_size},\n",
    "    {\"n\": 2, \"delta\": 0.1, \"smoothing\": \"laplace\", \"epochs\": 1, \"learning_rate\": 0.01, \"batch_size\": 32, \"context_size\": context_size},\n",
    "    {\"n\": 3, \"delta\": 0.1, \"smoothing\": \"laplace\", \"epochs\": 1, \"learning_rate\": 0.01, \"batch_size\": 32, \"context_size\": context_size},\n",
    "    {\"n\": 2, \"delta\": 0.1, \"smoothing\": \"backoff\", \"epochs\": 1, \"learning_rate\": 0.01, \"batch_size\": 32, \"context_size\": context_size},\n",
    "    {\"n\": 3, \"delta\": 0.1, \"smoothing\": \"interpolation\", \"epochs\": 1, \"learning_rate\": 0.01, \"batch_size\": 32, \"context_size\": context_size}\n",
    "]\n",
    "\n",
    "# keep only one GPT model\n",
    "gpt_configs = [\n",
    "    {\n",
    "        \"name\": \"tiny\",\n",
    "        \"vocab_size\": len(tokenizer.vocab),\n",
    "        \"n_embd\": 64,\n",
    "        \"n_head\": 4,\n",
    "        \"n_layer\": 4,\n",
    "        \"context_size\": context_size,\n",
    "        \"epochs\": 1,\n",
    "        \"learning_rate\": 3e-4,\n",
    "        \"batch_size\": 32,\n",
    "    }\n",
    "    # remove the \"small\" configuration\n",
    "]\n",
    "\n",
    "# keep only one Neural N-Gram model\n",
    "neural_ngram_configs = [\n",
    "    {\n",
    "        \"name\": \"neural_bigram\",\n",
    "        \"vocab_size\": len(tokenizer.vocab),\n",
    "        \"context_size\": 2,\n",
    "        \"embedding_dim\": 64,\n",
    "        \"hidden_dim\": 128,\n",
    "        \"epochs\": 1,\n",
    "        \"learning_rate\": 3e-4,\n",
    "        \"batch_size\": 32,\n",
    "    }\n",
    "    # remove the \"neural_trigram\" configuration\n",
    "]\n",
    "#---------For bigger experiment----------------\n",
    "# # N-gram configurations\n",
    "# ngram_configs = [\n",
    "#     {\"n\": 1, \"delta\": 0.1, \"smoothing\": \"laplace\", \"epochs\": 1, \"learning_rate\": 0.01, \"batch_size\": 32, \"context_size\": context_size},\n",
    "#     {\"n\": 2, \"delta\": 0.1, \"smoothing\": \"laplace\", \"epochs\": 1, \"learning_rate\": 0.01, \"batch_size\": 32, \"context_size\": context_size},\n",
    "#     {\"n\": 3, \"delta\": 0.1, \"smoothing\": \"laplace\", \"epochs\": 1, \"learning_rate\": 0.01, \"batch_size\": 32, \"context_size\": context_size},\n",
    "#     {\"n\": 2, \"delta\": 0.1, \"smoothing\": \"backoff\", \"epochs\": 1, \"learning_rate\": 0.01, \"batch_size\": 32, \"context_size\": context_size},\n",
    "#     {\"n\": 3, \"delta\": 0.1, \"smoothing\": \"interpolation\", \"epochs\": 1, \"learning_rate\": 0.01, \"batch_size\": 32, \"context_size\": context_size}\n",
    "# ]\n",
    "\n",
    "# # Mini-GPT configurations\n",
    "# gpt_configs = [\n",
    "#     {\n",
    "#         \"name\": \"tiny\",\n",
    "#         \"vocab_size\": len(tokenizer.vocab),\n",
    "#         \"n_embd\": 64,\n",
    "#         \"n_head\": 4,\n",
    "#         \"n_layer\": 4,\n",
    "#         \"context_size\": context_size,\n",
    "#         \"epochs\": 1,\n",
    "#         \"learning_rate\": 3e-4,\n",
    "#         \"batch_size\": 32,\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"small\",\n",
    "#         \"vocab_size\": len(tokenizer.vocab),\n",
    "#         \"n_embd\": 128,\n",
    "#         \"n_head\": 8,\n",
    "#         \"n_layer\": 6,\n",
    "#         \"context_size\": context_size,\n",
    "#         \"epochs\": 1,\n",
    "#         \"learning_rate\": 3e-4,\n",
    "#         \"batch_size\": 32,\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# # Neural N-Gram configurations\n",
    "# neural_ngram_configs = [\n",
    "#     {\n",
    "#         \"name\": \"neural_bigram\",\n",
    "#         \"vocab_size\": len(tokenizer.vocab),\n",
    "#         \"context_size\": 2,  # For a bigram model\n",
    "#         \"embedding_dim\": 64,\n",
    "#         \"hidden_dim\": 128,\n",
    "#         \"epochs\": 1,\n",
    "#         \"learning_rate\": 3e-4,\n",
    "#         \"batch_size\": 32,\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"neural_trigram\",\n",
    "#         \"vocab_size\": len(tokenizer.vocab),\n",
    "#         \"context_size\": 3,  # For a trigram model\n",
    "#         \"embedding_dim\": 128,\n",
    "#         \"hidden_dim\": 256,\n",
    "#         \"epochs\": 1,\n",
    "#         \"learning_rate\": 3e-4,\n",
    "#         \"batch_size\": 32,\n",
    "#     }\n",
    "# ]\n",
    "#-----------------------------------------------------------------\n",
    "def update_vocab_sizes(configs_list, vocab_size):\n",
    "    \"\"\"Create copies of configs with updated vocab size.\"\"\"\n",
    "    updated_configs = []\n",
    "    for config in configs_list:\n",
    "        new_config = config.copy()  # Create a copy\n",
    "        new_config['vocab_size'] = vocab_size  # Update the copy\n",
    "        updated_configs.append(new_config)\n",
    "    return updated_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8e497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ngram_experiments(tokenizer, train_stream, val_stream, ngram_configs):\n",
    "    \"\"\"Run N-gram experiments with given tokenizer.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for config in ngram_configs:\n",
    "        print(f\"\\nTraining N-gram model with n={config['n']}, smoothing={config['smoothing']}...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # create the model\n",
    "        model = NGramLM(\n",
    "            n=config['n'], \n",
    "            delta=config['delta'], \n",
    "            vocab_size=len(tokenizer.vocab),\n",
    "            smoothing_method=config['smoothing']\n",
    "        )\n",
    "        \n",
    "        # use the trainer for n-gram models\n",
    "        trainer_config = {\n",
    "            'epochs': config['epochs'],\n",
    "            'learning_rate': config['learning_rate'],\n",
    "            'batch_size': config['batch_size'],\n",
    "            'use_mixed_precision': False, \n",
    "            'context_size': config['context_size']\n",
    "        }\n",
    "            \n",
    "        experiment_dir = Path(f\"../experiments/ngram_{config['n']}_{config['smoothing']}\")\n",
    "        experiment_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        trainer = ModelTrainer(\n",
    "            model_type=\"ngram\",\n",
    "            config=trainer_config,\n",
    "            experiment_dir=experiment_dir\n",
    "        )\n",
    "        \n",
    "        # train using the trainer\n",
    "        model, metrics = trainer.train(model, train_stream, val_stream)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        # evaluate the model\n",
    "        train_ppl = model.perplexity(train_stream)\n",
    "        val_ppl = model.perplexity(val_stream)\n",
    "        \n",
    "        # generate a sample\n",
    "        seed_text = \"The\"\n",
    "        prefix = tokenizer.encode(seed_text, add_bos=False, add_eos=False)\n",
    "        generated_ids = model.generate_text(\n",
    "            prefix=prefix,\n",
    "            max_length=30,\n",
    "            temperature=0.8,\n",
    "            eos_token=tokenizer.inverse_vocab[\"<EOS>\"]\n",
    "        )\n",
    "        try:\n",
    "            generated_text = tokenizer.decode(generated_ids)\n",
    "        except UnicodeDecodeError:\n",
    "            generated_text = f\"<Unicode decode error: invalid byte sequence in generated tokens>\"\n",
    "        \n",
    "        # store results\n",
    "        result = {\n",
    "            \"type\": \"N-gram\",\n",
    "            \"config\": f\"{config['n']}-gram ({config['smoothing']})\",\n",
    "            \"train_time\": train_time,\n",
    "            \"train_ppl\": train_ppl,\n",
    "            \"val_ppl\": val_ppl,\n",
    "            \"sample\": generated_text\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"  Training time: {train_time:.2f}s\")\n",
    "        print(f\"  Train perplexity: {train_ppl:.2f}\")\n",
    "        print(f\"  Val perplexity: {val_ppl:.2f}\")\n",
    "        print(f\"  Generated text: '{generated_text}'\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_neural_ngram_experiments(tokenizer, x_train, y_train, x_val, y_val, neural_ngram_configs):\n",
    "    \"\"\"Run Neural N-gram experiments with given tokenizer.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for config in neural_ngram_configs:\n",
    "        print(f\"\\nTraining Neural N-Gram model: {config['name']}...\")\n",
    "        \n",
    "        # initialize model\n",
    "        model = NeuralNGramModel(\n",
    "            vocab_size=config['vocab_size'],\n",
    "            context_size=config['context_size'],\n",
    "            embedding_dim=config['embedding_dim'],\n",
    "            hidden_dim=config['hidden_dim'],\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        model = model.to(device)\n",
    "\n",
    "        # setup trainer\n",
    "        trainer_config = {\n",
    "            'epochs': config['epochs'],\n",
    "            'learning_rate': config['learning_rate'],\n",
    "            'batch_size': config['batch_size'],\n",
    "            'use_mixed_precision': False,\n",
    "            'context_size': config['context_size']\n",
    "        }\n",
    "        \n",
    "        experiment_dir = Path(f\"../experiments/neural_ngram_{config['name']}\")\n",
    "        experiment_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        trainer = ModelTrainer(\n",
    "            model_type=\"neural\",\n",
    "            config=trainer_config,\n",
    "            experiment_dir=experiment_dir\n",
    "        )\n",
    "        \n",
    "        # prepare data\n",
    "        from torch.utils.data import TensorDataset, DataLoader\n",
    "        \n",
    "        def prepare_ngram_data(x, y, context_size):\n",
    "            if x.size(1) > context_size:\n",
    "                inputs = x[:, -context_size:]\n",
    "            else:\n",
    "                inputs = x\n",
    "            targets = y[:, -1]\n",
    "            return TensorDataset(inputs, targets)\n",
    "        \n",
    "        train_dataset = prepare_ngram_data(x_train, y_train, config['context_size'])\n",
    "        val_dataset = prepare_ngram_data(x_val, y_val, config['context_size'])\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=config['batch_size'])\n",
    "        \n",
    "        # train model\n",
    "        start_time = time.time()\n",
    "        model, metrics = trainer.train(model, train_loader, val_loader)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        # get final metrics\n",
    "        final_train_metrics = metrics['train_metrics'][-1] if metrics['train_metrics'] else {'loss': float('nan')}\n",
    "        final_val_metrics = metrics['val_metrics'][-1] if metrics['val_metrics'] else {'loss': float('nan'), 'perplexity': float('nan')}\n",
    "        \n",
    "        # generate sample\n",
    "        try:\n",
    "            model.eval()\n",
    "            prefix = tokenizer.encode(\"The\", add_bos=False, add_eos=False)\n",
    "            \n",
    "            if len(prefix) >= model.context_size:\n",
    "                input_tokens = prefix[-model.context_size:]\n",
    "            else:\n",
    "                padding_needed = model.context_size - len(prefix)\n",
    "                input_tokens = [0] * padding_needed + prefix\n",
    "            \n",
    "            input_ids = torch.tensor([input_tokens], dtype=torch.long).to(next(model.parameters()).device)\n",
    "            generated_ids = model.generate(input_ids, max_new_tokens=30, temperature=0.8, top_k=50)\n",
    "            sample = tokenizer.decode(generated_ids[0].tolist())\n",
    "        except Exception as e:\n",
    "            sample = f\"<Generation failed: {str(e)}>\"\n",
    "        \n",
    "        # store results\n",
    "        result = {\n",
    "            \"type\": \"Neural N-Gram\",\n",
    "            \"config\": f\"{config['name']} (ctx={config['context_size']}, emb={config['embedding_dim']})\",\n",
    "            \"train_time\": train_time,\n",
    "            \"train_ppl\": torch.exp(torch.tensor(final_train_metrics['loss'])).item(),\n",
    "            \"val_ppl\": final_val_metrics['perplexity'],\n",
    "            \"sample\": sample\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"  Training time: {train_time:.2f}s\")\n",
    "        print(f\"  Train perplexity: {torch.exp(torch.tensor(final_train_metrics['loss'])).item():.2f}\")\n",
    "        print(f\"  Val perplexity: {final_val_metrics['perplexity']:.2f}\")\n",
    "        print(f\"  Generated text: '{sample}'\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_gpt_experiments(tokenizer, x_train, y_train, x_val, y_val, gpt_configs):\n",
    "    \"\"\"Run Mini-GPT experiments with given tokenizer.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for config in gpt_configs:\n",
    "        print(f\"\\nTraining Mini-GPT model: {config['name']}...\")\n",
    "        \n",
    "        # initialize model\n",
    "        model = MiniGPT(\n",
    "            vocab_size=config['vocab_size'],\n",
    "            embedding_dim=config['n_embd'],  \n",
    "            context_size=config['context_size'],\n",
    "            n_layers=config['n_layer'],      \n",
    "            n_heads=config['n_head'],        \n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        model = model.to(device)\n",
    "\n",
    "        # setup trainer\n",
    "        trainer_config = {\n",
    "            'epochs': config['epochs'],\n",
    "            'learning_rate': config['learning_rate'],\n",
    "            'batch_size': config['batch_size'],\n",
    "            'use_mixed_precision': True,\n",
    "            'context_size': config['context_size']\n",
    "        }\n",
    "        \n",
    "        experiment_dir = Path(f\"../experiments/mini_gpt_{config['name']}\")\n",
    "        experiment_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        trainer = ModelTrainer(\n",
    "            model_type=\"neural\",\n",
    "            config=trainer_config,\n",
    "            experiment_dir=experiment_dir\n",
    "        )\n",
    "        \n",
    "        # create datasets\n",
    "        from torch.utils.data import TensorDataset, DataLoader\n",
    "        train_dataset = TensorDataset(x_train, y_train)\n",
    "        val_dataset = TensorDataset(x_val, y_val)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=config['batch_size'])\n",
    "        \n",
    "        # train model\n",
    "        start_time = time.time()\n",
    "        model, metrics = trainer.train(model, train_loader, val_loader)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        # get final metrics\n",
    "        final_train_metrics = metrics['train_metrics'][-1] if metrics['train_metrics'] else {'loss': float('nan')}\n",
    "        final_val_metrics = metrics['val_metrics'][-1] if metrics['val_metrics'] else {'loss': float('nan'), 'perplexity': float('nan')}\n",
    "        \n",
    "        # generate sample\n",
    "        try:\n",
    "            model.eval()\n",
    "            prefix = tokenizer.encode(\"The\", add_bos=False, add_eos=False)\n",
    "            input_ids = torch.tensor([prefix], dtype=torch.long).to(next(model.parameters()).device)\n",
    "            \n",
    "            if hasattr(model, 'generate'):\n",
    "                generated_ids = model.generate(input_ids, max_new_tokens=30, temperature=0.8, top_k=50)\n",
    "                sample = tokenizer.decode(generated_ids[0].tolist())\n",
    "            else:\n",
    "                # fallback generation\n",
    "                with torch.no_grad():\n",
    "                    for _ in range(30):\n",
    "                        outputs = model(input_ids)\n",
    "                        next_token_logits = outputs[:, -1, :]\n",
    "                        probs = torch.nn.functional.softmax(next_token_logits / 0.8, dim=-1)\n",
    "                        next_token = torch.multinomial(probs, num_samples=1)\n",
    "                        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "                        \n",
    "                        if hasattr(tokenizer, 'eos_id') and next_token.item() == tokenizer.eos_id:\n",
    "                            break\n",
    "                            \n",
    "                sample = tokenizer.decode(input_ids[0].tolist())\n",
    "        except Exception as e:\n",
    "            sample = f\"<Generation failed: {str(e)}>\"\n",
    "        \n",
    "        # store results\n",
    "        result = {\n",
    "            \"type\": \"Mini-GPT\",\n",
    "            \"config\": f\"{config['name']} (emb={config['n_embd']}, layers={config['n_layer']})\",\n",
    "            \"train_time\": train_time,\n",
    "            \"train_ppl\": torch.exp(torch.tensor(final_train_metrics['loss'])).item(),\n",
    "            \"val_ppl\": final_val_metrics['perplexity'],\n",
    "            \"sample\": sample\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"  Training time: {train_time:.2f}s\")\n",
    "        print(f\"  Train perplexity: {torch.exp(torch.tensor(final_train_metrics['loss'])).item():.2f}\")\n",
    "        print(f\"  Val perplexity: {final_val_metrics['perplexity']:.2f}\")\n",
    "        print(f\"  Generated text: '{sample}'\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdd23c9",
   "metadata": {},
   "source": [
    "# Helper Functions for Multi-Tokenizer Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc2bd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stage 1: Find Best Tokenizers for N-Gram Models\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STAGE 1: TESTING ALL TOKENIZERS ON N-GRAM MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Stage 1: Test all tokenizers on N-gram models only\n",
    "ngram_tokenizer_results = []\n",
    "tokenizer_performance = {}\n",
    "\n",
    "for merges, tokenizer in tokenizers.items():\n",
    "    print(f\"\\n--- Testing {merges}-merge tokenizer on N-gram models ---\")\n",
    "    print(f\"Vocabulary size: {len(tokenizer.vocab)}\")\n",
    "    \n",
    "    # prepare data for this tokenizer\n",
    "    train_stream = build_stream(tokenizer, train_lines)\n",
    "    val_stream = build_stream(tokenizer, val_lines)\n",
    "    \n",
    "    # update n-gram configs with new vocab size\n",
    "    current_ngram_configs = update_vocab_sizes(ngram_configs, len(tokenizer.vocab))\n",
    "    \n",
    "    # run N-gram experiments only\n",
    "    ngram_results = run_ngram_experiments(tokenizer, train_stream, val_stream, current_ngram_configs)\n",
    "\n",
    "    # add tokenizer info to results\n",
    "    for result in ngram_results:\n",
    "        result['tokenizer_merges'] = merges\n",
    "        result['vocab_size'] = len(tokenizer.vocab)\n",
    "    \n",
    "    # store results\n",
    "    ngram_tokenizer_results.extend(ngram_results)\n",
    "    \n",
    "    # calculate average validation perplexity for this tokenizer\n",
    "    avg_val_ppl = sum([r['val_ppl'] for r in ngram_results]) / len(ngram_results)\n",
    "    tokenizer_performance[merges] = {\n",
    "        'avg_val_ppl': avg_val_ppl,\n",
    "        'vocab_size': len(tokenizer.vocab),\n",
    "        'results': ngram_results\n",
    "    }\n",
    "    \n",
    "    print(f\"Average validation perplexity for {merges}-merge tokenizer: {avg_val_ppl:.2f}\")\n",
    "\n",
    "# find top 3 tokenizers based on N-gram performance\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FINDING TOP-3 TOKENIZERS FOR N-GRAM MODELS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# sort tokenizers by average validation perplexity (lower is better)\n",
    "sorted_tokenizers = sorted(tokenizer_performance.items(), \n",
    "                          key=lambda x: x[1]['avg_val_ppl'])\n",
    "\n",
    "top_3_merges = [merges for merges, _ in sorted_tokenizers[:3]]\n",
    "print(\"Tokenizer performance ranking (by avg validation perplexity):\")\n",
    "for i, (merges, perf) in enumerate(sorted_tokenizers, 1):\n",
    "    marker = \"‚≠ê\" if merges in top_3_merges else \"  \"\n",
    "    print(f\"{marker} {i}. {merges} merges (vocab={perf['vocab_size']}): {perf['avg_val_ppl']:.2f}\")\n",
    "\n",
    "print(f\"\\nüèÜ Top-3 tokenizers selected for GPT testing: {top_3_merges}\")\n",
    "\n",
    "# create DataFrame for N-gram stage results\n",
    "ngram_stage_df = pd.DataFrame(ngram_tokenizer_results)\n",
    "print(f\"\\nStage 1 completed: {len(ngram_stage_df)} N-gram experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a56d8a",
   "metadata": {},
   "source": [
    "# Multi-Tokenizer Experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff627c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RUNNING EXPERIMENTS WITH 200-MERGE TOKENIZER\n",
      "Vocabulary size: 458\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 22:04:20,384 - INFO - Efficient attention enabled\n",
      "2025-08-31 22:04:20,385 - INFO - Initialized trainer on mps\n",
      "2025-08-31 22:04:20,385 - INFO - Starting training for ngram model\n",
      "2025-08-31 22:04:20,385 - INFO - Training classical n-gram model...\n",
      "2025-08-31 22:04:20,572 - INFO - Validation perplexity: 160.905\n",
      "2025-08-31 22:04:20,572 - INFO - N-gram training completed in 0.19 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- N-Gram Models ---\n",
      "\n",
      "Training N-gram model with n=1, smoothing=laplace...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 22:04:21,157 - INFO - Efficient attention enabled\n",
      "2025-08-31 22:04:21,157 - INFO - Initialized trainer on mps\n",
      "2025-08-31 22:04:21,157 - INFO - Starting training for ngram model\n",
      "2025-08-31 22:04:21,158 - INFO - Training classical n-gram model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training time: 0.19s\n",
      "  Train perplexity: 160.54\n",
      "  Val perplexity: 160.91\n",
      "  Generated text: 'the wi&ouytheat your7e. 'x you&she 's x ofo.  seck wf this of9:qu'\n",
      "\n",
      "Training N-gram model with n=2, smoothing=laplace...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 22:04:21,738 - INFO - Validation perplexity: 41.678\n",
      "2025-08-31 22:04:21,738 - INFO - N-gram training completed in 0.58 seconds\n",
      "2025-08-31 22:04:22,605 - INFO - Efficient attention enabled\n",
      "2025-08-31 22:04:22,606 - INFO - Initialized trainer on mps\n",
      "2025-08-31 22:04:22,606 - INFO - Starting training for ngram model\n",
      "2025-08-31 22:04:22,606 - INFO - Training classical n-gram model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training time: 0.58s\n",
      "  Train perplexity: 40.76\n",
      "  Val perplexity: 41.68\n",
      "  Generated text: 'thee compl froud with by sterswhat ive to your hervein so sha'\n",
      "\n",
      "Training N-gram model with n=3, smoothing=laplace...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 22:04:23,495 - INFO - Validation perplexity: 21.987\n",
      "2025-08-31 22:04:23,496 - INFO - N-gram training completed in 0.89 seconds\n",
      "2025-08-31 22:04:24,573 - INFO - Efficient attention enabled\n",
      "2025-08-31 22:04:24,573 - INFO - Initialized trainer on mps\n",
      "2025-08-31 22:04:24,573 - INFO - Starting training for ngram model\n",
      "2025-08-31 22:04:24,574 - INFO - Training classical n-gram model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training time: 0.89s\n",
      "  Train perplexity: 16.81\n",
      "  Val perplexity: 21.99\n",
      "  Generated text: '<Unicode decode error: invalid byte sequence in generated tokens>'\n",
      "\n",
      "Training N-gram model with n=2, smoothing=backoff...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 22:04:25,136 - INFO - Validation perplexity: 41.306\n",
      "2025-08-31 22:04:25,137 - INFO - N-gram training completed in 0.56 seconds\n",
      "2025-08-31 22:04:25,806 - INFO - Efficient attention enabled\n",
      "2025-08-31 22:04:25,807 - INFO - Initialized trainer on mps\n",
      "2025-08-31 22:04:25,807 - INFO - Starting training for ngram model\n",
      "2025-08-31 22:04:25,807 - INFO - Training classical n-gram model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training time: 0.57s\n",
      "  Train perplexity: 40.57\n",
      "  Val perplexity: 41.31\n",
      "  Generated text: 'thee to be dition! whe) ress, we' le, and gn, and be dish'd so '\n",
      "\n",
      "Training N-gram model with n=3, smoothing=interpolation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 22:04:26,525 - INFO - Validation perplexity: 43.723\n",
      "2025-08-31 22:04:26,525 - INFO - N-gram training completed in 0.72 seconds\n",
      "2025-08-31 22:04:28,070 - INFO - Efficient attention enabled\n",
      "2025-08-31 22:04:28,070 - INFO - Initialized trainer on mps\n",
      "2025-08-31 22:04:28,071 - INFO - Starting training for neural model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training time: 0.72s\n",
      "  Train perplexity: 36.04\n",
      "  Val perplexity: 43.72\n",
      "  Generated text: '<Unicode decode error: invalid byte sequence in generated tokens>'\n",
      "\n",
      "--- Neural N-Gram Models ---\n",
      "\n",
      "Training Neural N-Gram model: neural_bigram...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 22:04:28,623 - INFO - Model compiled for better performance\n",
      "2025-08-31 22:04:28,624 - INFO - Epoch 1/1\n",
      "2025-08-31 22:09:01,860 - INFO - Epoch 1: train_loss=3.9155, val_loss=3.6718, val_ppl=39.32\n",
      "2025-08-31 22:09:02,011 - INFO - Efficient attention enabled\n",
      "2025-08-31 22:09:02,011 - INFO - Initialized trainer on mps\n",
      "2025-08-31 22:09:02,012 - INFO - Starting training for neural model\n",
      "2025-08-31 22:09:02,013 - INFO - Model compiled for better performance\n",
      "2025-08-31 22:09:02,013 - INFO - Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training time: 273.79s\n",
      "  Train perplexity: 50.17\n",
      "  Val perplexity: 39.32\n",
      "  Generated text: 'they you charg, and dove trich, whenter most then, moa. prac'\n",
      "\n",
      "Training Neural N-Gram model: neural_trigram...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 22:14:20,060 - INFO - Epoch 1: train_loss=3.6142, val_loss=3.4046, val_ppl=30.10\n",
      "2025-08-31 22:14:20,429 - INFO - Mixed precision training enabled\n",
      "2025-08-31 22:14:20,430 - INFO - Efficient attention enabled\n",
      "2025-08-31 22:14:20,430 - INFO - Initialized trainer on mps\n",
      "2025-08-31 22:14:20,430 - INFO - Starting training for neural model\n",
      "2025-08-31 22:14:20,432 - INFO - Model compiled for better performance\n",
      "2025-08-31 22:14:20,432 - INFO - Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training time: 318.05s\n",
      "  Train perplexity: 37.12\n",
      "  Val perplexity: 30.10\n",
      "  Generated text: 'they forbe, of the mispi. an may! it be done! ky; these '\n",
      "\n",
      "--- Mini-GPT Models ---\n",
      "\n",
      "Training Mini-GPT model: tiny...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 22:32:54,106 - INFO - Epoch 1: train_loss=3.1318, val_loss=2.8282, val_ppl=16.91\n",
      "2025-08-31 22:32:54,621 - INFO - Mixed precision training enabled\n",
      "2025-08-31 22:32:54,621 - INFO - Efficient attention enabled\n",
      "2025-08-31 22:32:54,621 - INFO - Initialized trainer on mps\n",
      "2025-08-31 22:32:54,621 - INFO - Starting training for neural model\n",
      "2025-08-31 22:32:54,623 - INFO - Model compiled for better performance\n",
      "2025-08-31 22:32:54,623 - INFO - Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training time: 1113.68s\n",
      "  Train perplexity: 22.92\n",
      "  Val perplexity: 16.91\n",
      "  Generated text: 'thee before he that tooke; when shall be my gracious hands from h'\n",
      "\n",
      "Training Mini-GPT model: small...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 23:12:22,477 - INFO - Epoch 1: train_loss=2.6448, val_loss=2.4960, val_ppl=12.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training time: 2367.86s\n",
      "  Train perplexity: 14.08\n",
      "  Val perplexity: 12.13\n",
      "  Generated text: 'they doth. armado. [aside] and whatsoever lives asid'\n",
      "\n",
      "============================================================\n",
      "RUNNING EXPERIMENTS WITH 500-MERGE TOKENIZER\n",
      "Vocabulary size: 758\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 23:14:47,365 - INFO - Efficient attention enabled\n",
      "2025-08-31 23:14:47,366 - INFO - Initialized trainer on mps\n",
      "2025-08-31 23:14:47,366 - INFO - Starting training for ngram model\n",
      "2025-08-31 23:14:47,366 - INFO - Training classical n-gram model...\n",
      "2025-08-31 23:14:47,497 - INFO - Validation perplexity: 339.644\n",
      "2025-08-31 23:14:47,498 - INFO - N-gram training completed in 0.13 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- N-Gram Models ---\n",
      "\n",
      "Training N-gram model with n=1, smoothing=laplace...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 23:14:47,898 - INFO - Efficient attention enabled\n",
      "2025-08-31 23:14:47,898 - INFO - Initialized trainer on mps\n",
      "2025-08-31 23:14:47,898 - INFO - Starting training for ngram model\n",
      "2025-08-31 23:14:47,898 - INFO - Training classical n-gram model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training time: 0.13s\n",
      "  Train perplexity: 337.25\n",
      "  Val perplexity: 339.64\n",
      "  Generated text: 'thereprothingtilogentdo resa. s earwhgood es gheboelfimake enot par py in loo?  their'\n",
      "\n",
      "Training N-gram model with n=2, smoothing=laplace...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 23:14:48,252 - INFO - Validation perplexity: 63.269\n",
      "2025-08-31 23:14:48,252 - INFO - N-gram training completed in 0.35 seconds\n",
      "2025-08-31 23:14:48,725 - INFO - Efficient attention enabled\n",
      "2025-08-31 23:14:48,725 - INFO - Initialized trainer on mps\n",
      "2025-08-31 23:14:48,725 - INFO - Starting training for ngram model\n",
      "2025-08-31 23:14:48,726 - INFO - Training classical n-gram model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training time: 0.35s\n",
      "  Train perplexity: 57.82\n",
      "  Val perplexity: 63.27\n",
      "  Generated text: 'thee. garning into the prince, that weland donegree wit; and now, et at dead, si'\n",
      "\n",
      "Training N-gram model with n=3, smoothing=laplace...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 23:14:49,219 - INFO - Validation perplexity: 54.383\n",
      "2025-08-31 23:14:49,220 - INFO - N-gram training completed in 0.49 seconds\n",
      "2025-08-31 23:14:49,819 - INFO - Efficient attention enabled\n",
      "2025-08-31 23:14:49,819 - INFO - Initialized trainer on mps\n",
      "2025-08-31 23:14:49,819 - INFO - Starting training for ngram model\n",
      "2025-08-31 23:14:49,819 - INFO - Training classical n-gram model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training time: 0.50s\n",
      "  Train perplexity: 27.07\n",
      "  Val perplexity: 54.38\n",
      "  Generated text: '<Unicode decode error: invalid byte sequence in generated tokens>'\n",
      "\n",
      "Training N-gram model with n=2, smoothing=backoff...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 23:14:50,172 - INFO - Validation perplexity: 60.720\n",
      "2025-08-31 23:14:50,172 - INFO - N-gram training completed in 0.35 seconds\n",
      "2025-08-31 23:14:50,617 - INFO - Efficient attention enabled\n",
      "2025-08-31 23:14:50,617 - INFO - Initialized trainer on mps\n",
      "2025-08-31 23:14:50,618 - INFO - Starting training for ngram model\n",
      "2025-08-31 23:14:50,618 - INFO - Training classical n-gram model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training time: 0.36s\n",
      "  Train perplexity: 56.63\n",
      "  Val perplexity: 60.72\n",
      "  Generated text: 'thee, he hath birst and say-gownloy; bring upon thy houd. ere is not so ill it '\n",
      "\n",
      "Training N-gram model with n=3, smoothing=interpolation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 23:14:51,153 - INFO - Validation perplexity: 75.312\n",
      "2025-08-31 23:14:51,154 - INFO - N-gram training completed in 0.54 seconds\n",
      "2025-08-31 23:14:52,359 - INFO - Efficient attention enabled\n",
      "2025-08-31 23:14:52,359 - INFO - Initialized trainer on mps\n",
      "2025-08-31 23:14:52,360 - INFO - Starting training for neural model\n",
      "2025-08-31 23:14:52,362 - INFO - Model compiled for better performance\n",
      "2025-08-31 23:14:52,362 - INFO - Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training time: 0.54s\n",
      "  Train perplexity: 39.91\n",
      "  Val perplexity: 75.31\n",
      "  Generated text: '<Unicode decode error: invalid byte sequence in generated tokens>'\n",
      "\n",
      "--- Neural N-Gram Models ---\n",
      "\n",
      "Training Neural N-Gram model: neural_bigram...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m ngram_results = run_ngram_experiments(tokenizer, train_stream, val_stream, ngram_configs)\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Neural N-Gram Models ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m neural_ngram_results = \u001b[43mrun_neural_ngram_experiments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneural_ngram_configs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Mini-GPT Models ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m gpt_results = run_gpt_experiments(tokenizer, x_train, y_train, x_val, y_val, gpt_configs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 130\u001b[39m, in \u001b[36mrun_neural_ngram_experiments\u001b[39m\u001b[34m(tokenizer, x_train, y_train, x_val, y_val, neural_ngram_configs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m    129\u001b[39m start_time = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m model, metrics = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m train_time = time.time() - start_time\n\u001b[32m    133\u001b[39m \u001b[38;5;66;03m# Get final metrics\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Workspace/01_University/Masters/01_Semestr/03_GPT_Scratch/gpt_from_scratch/src/gpt_from_scratch/trainer/trainer.py:136\u001b[39m, in \u001b[36mModelTrainer.train\u001b[39m\u001b[34m(self, model, train_data, val_data)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._train_ngram(model, train_data, val_data)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_neural\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Workspace/01_University/Masters/01_Semestr/03_GPT_Scratch/gpt_from_scratch/src/gpt_from_scratch/trainer/trainer.py:184\u001b[39m, in \u001b[36mModelTrainer._train_neural\u001b[39m\u001b[34m(self, model, train_data, val_data)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28mself\u001b[39m.logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    183\u001b[39m \u001b[38;5;66;03m# Train one epoch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m train_metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[38;5;28mself\u001b[39m.train_metrics.append(train_metrics)\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# Validate\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Workspace/01_University/Masters/01_Semestr/03_GPT_Scratch/gpt_from_scratch/src/gpt_from_scratch/trainer/trainer.py:258\u001b[39m, in \u001b[36mModelTrainer._train_epoch\u001b[39m\u001b[34m(self, model, optimizer, data_loader)\u001b[39m\n\u001b[32m    255\u001b[39m     loss.backward()\n\u001b[32m    257\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.get(\u001b[33m'\u001b[39m\u001b[33mclip_grad\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m         \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmax_grad_norm\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    263\u001b[39m     optimizer.step()\n\u001b[32m    265\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Workspace/01_University/Masters/01_Semestr/03_GPT_Scratch/gpt_from_scratch/gptscratch/lib/python3.13/site-packages/torch/nn/utils/clip_grad.py:36\u001b[39m, in \u001b[36m_no_grad.<locals>._no_grad_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_no_grad_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Workspace/01_University/Masters/01_Semestr/03_GPT_Scratch/gpt_from_scratch/gptscratch/lib/python3.13/site-packages/torch/nn/utils/clip_grad.py:221\u001b[39m, in \u001b[36mclip_grad_norm_\u001b[39m\u001b[34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[39m\n\u001b[32m    216\u001b[39m         warnings.warn(\n\u001b[32m    217\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`parameters` is an empty generator, no gradient clipping will occur.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    218\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    219\u001b[39m         )\n\u001b[32m    220\u001b[39m grads = [p.grad \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters \u001b[38;5;28;01mif\u001b[39;00m p.grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m total_norm = \u001b[43m_get_total_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_if_nonfinite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m _clip_grads_with_norm_(parameters, max_norm, total_norm, foreach)\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m total_norm\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Workspace/01_University/Masters/01_Semestr/03_GPT_Scratch/gpt_from_scratch/gptscratch/lib/python3.13/site-packages/torch/nn/utils/clip_grad.py:36\u001b[39m, in \u001b[36m_no_grad.<locals>._no_grad_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_no_grad_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Workspace/01_University/Masters/01_Semestr/03_GPT_Scratch/gpt_from_scratch/gptscratch/lib/python3.13/site-packages/torch/nn/utils/clip_grad.py:42\u001b[39m, in \u001b[36m_get_total_norm\u001b[39m\u001b[34m(tensors, norm_type, error_if_nonfinite, foreach)\u001b[39m\n\u001b[32m     38\u001b[39m     functools.update_wrapper(_no_grad_wrapper, func)\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _no_grad_wrapper\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[38;5;129m@_no_grad\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_total_norm\u001b[39m(\n\u001b[32m     44\u001b[39m     tensors: _tensor_or_tensors,\n\u001b[32m     45\u001b[39m     norm_type: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m2.0\u001b[39m,\n\u001b[32m     46\u001b[39m     error_if_nonfinite: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     47\u001b[39m     foreach: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     48\u001b[39m ) -> torch.Tensor:\n\u001b[32m     49\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Compute the norm of an iterable of tensors.\u001b[39;00m\n\u001b[32m     50\u001b[39m \n\u001b[32m     51\u001b[39m \u001b[33;03m    The norm is computed over the norms of the individual tensors, as if the norms of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     68\u001b[39m \u001b[33;03m        Total norm of the tensors (viewed as a single vector).\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch.Tensor):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# run experiments across all tokenizers and models\n",
    "all_results = []\n",
    "tokenizer_results = {}\n",
    "\n",
    "for merges, tokenizer in tokenizers.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RUNNING EXPERIMENTS WITH {merges}-MERGE TOKENIZER\")\n",
    "    print(f\"Vocabulary size: {len(tokenizer.vocab)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # prepare data for this tokenizer\n",
    "    train_stream = build_stream(tokenizer, train_lines)\n",
    "    val_stream = build_stream(tokenizer, val_lines)\n",
    "    x_train, y_train = create_tensor_dataset(train_stream, context_size)\n",
    "    x_val, y_val = create_tensor_dataset(val_stream, context_size)\n",
    "    \n",
    "    # update configs with new vocab size\n",
    "    current_ngram_configs = update_vocab_sizes(ngram_configs, len(tokenizer.vocab))\n",
    "    current_gpt_configs = update_vocab_sizes(gpt_configs, len(tokenizer.vocab))\n",
    "    current_neural_ngram_configs = update_vocab_sizes(neural_ngram_configs, len(tokenizer.vocab))\n",
    "    \n",
    "    # run all model experiments\n",
    "    print(f\"\\n--- N-Gram Models ---\")\n",
    "    ngram_results = run_ngram_experiments(tokenizer, train_stream, val_stream, current_ngram_configs)\n",
    "    \n",
    "    print(f\"\\n--- Neural N-Gram Models ---\")\n",
    "    neural_ngram_results = run_neural_ngram_experiments(tokenizer, x_train, y_train, x_val, y_val, current_neural_ngram_configs)\n",
    "    \n",
    "    print(f\"\\n--- Mini-GPT Models ---\")\n",
    "    gpt_results = run_gpt_experiments(tokenizer, x_train, y_train, x_val, y_val, current_gpt_configs)    \n",
    "    \n",
    "    # combine results for this tokenizer\n",
    "    tokenizer_results[merges] = {\n",
    "        'ngram': ngram_results,\n",
    "        'neural_ngram': neural_ngram_results, \n",
    "        'gpt': gpt_results\n",
    "    }\n",
    "    \n",
    "    # add tokenizer info to results\n",
    "    for result in ngram_results + neural_ngram_results + gpt_results:\n",
    "        result['tokenizer_merges'] = merges\n",
    "        result['vocab_size'] = len(tokenizer.vocab)\n",
    "    \n",
    "    # add to overall results\n",
    "    all_results.extend(ngram_results + neural_ngram_results + gpt_results)\n",
    "\n",
    "# convert to DataFrame\n",
    "all_results_df = pd.DataFrame(all_results)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total experiments: {len(all_results_df)}\")\n",
    "print(f\"Tokenizers tested: {list(tokenizers.keys())}\")\n",
    "print(f\"Model types: {all_results_df['type'].unique()}\")\n",
    "\n",
    "# display summary table\n",
    "summary_cols = ['tokenizer_merges', 'vocab_size', 'type', 'config', 'train_ppl', 'val_ppl', 'train_time']\n",
    "display(all_results_df[summary_cols].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104e460e",
   "metadata": {},
   "source": [
    "# Common Method for all architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770d844c",
   "metadata": {},
   "source": [
    "## Results Visualization\n",
    "\n",
    "Compare the performance of different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3974603f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_results_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Training perplexity\u001b[39;00m\n\u001b[32m     11\u001b[39m plt.subplot(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m sns.barplot(x=\u001b[33m'\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m'\u001b[39m, y=\u001b[33m'\u001b[39m\u001b[33mtrain_ppl\u001b[39m\u001b[33m'\u001b[39m, hue=\u001b[33m'\u001b[39m\u001b[33mtokenizer_merges\u001b[39m\u001b[33m'\u001b[39m, data=\u001b[43mall_results_df\u001b[49m)\n\u001b[32m     13\u001b[39m plt.title(\u001b[33m'\u001b[39m\u001b[33mTraining Perplexity by Tokenizer\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     14\u001b[39m plt.xlabel(\u001b[33m'\u001b[39m\u001b[33mModel Configuration\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'all_results_df' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# results Visualization\n",
    "\n",
    "# create figures directory if it doesn't exist\n",
    "figures_dir = Path(\"../figures\")\n",
    "figures_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# plot perplexity comparison with tokenizer info\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# training perplexity\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(x='config', y='train_ppl', hue='tokenizer_merges', data=all_results_df)\n",
    "plt.title('Training Perplexity by Tokenizer')\n",
    "plt.xlabel('Model Configuration')\n",
    "plt.ylabel('Perplexity (lower is better)')\n",
    "plt.yscale('log')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Tokenizer Merges')\n",
    "plt.tight_layout()\n",
    "\n",
    "# validation perplexity\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.barplot(x='config', y='val_ppl', hue='tokenizer_merges', data=all_results_df)\n",
    "plt.title('Validation Perplexity by Tokenizer')\n",
    "plt.xlabel('Model Configuration')\n",
    "plt.ylabel('Perplexity (lower is better)')\n",
    "plt.yscale('log')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Tokenizer Merges')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"../figures/perplexity_by_tokenizer_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# plot training time comparison\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(x='config', y='train_time', hue='tokenizer_merges', data=all_results_df)\n",
    "plt.title('Training Time Comparison by Tokenizer')\n",
    "plt.xlabel('Model Configuration')\n",
    "plt.ylabel('Training Time (seconds)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Tokenizer Merges')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../figures/training_time_by_tokenizer_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# create a vocabulary size vs perplexity plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.scatterplot(x='vocab_size', y='train_ppl', hue='type', style='tokenizer_merges', \n",
    "                size='tokenizer_merges', sizes=(50, 200), data=all_results_df)\n",
    "plt.title('Training Perplexity vs Vocabulary Size')\n",
    "plt.xlabel('Vocabulary Size')\n",
    "plt.ylabel('Training Perplexity')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(x='vocab_size', y='val_ppl', hue='type', style='tokenizer_merges',\n",
    "                size='tokenizer_merges', sizes=(50, 200), data=all_results_df)\n",
    "plt.title('Validation Perplexity vs Vocabulary Size')\n",
    "plt.xlabel('Vocabulary Size')\n",
    "plt.ylabel('Validation Perplexity')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../figures/perplexity_vs_vocab_size.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a16ca25",
   "metadata": {},
   "source": [
    "## Text Generation Comparison\n",
    "\n",
    "Compare text generation quality between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c8df97",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_results_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Display generated samples side by side\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m samples_df = \u001b[43mall_results_df\u001b[49m[[\u001b[33m'\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msample\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m      3\u001b[39m samples_df = samples_df.sort_values([\u001b[33m'\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGenerated Text Samples:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'all_results_df' is not defined"
     ]
    }
   ],
   "source": [
    "# display generated samples side by side\n",
    "samples_df = all_results_df[['type', 'config', 'sample']]\n",
    "samples_df = samples_df.sort_values(['type', 'config'])\n",
    "\n",
    "print(\"Generated Text Samples:\")\n",
    "for _, row in samples_df.iterrows():\n",
    "    print(f\"\\n{row['type']} - {row['config']}:\")\n",
    "    print(f\"  \\\"{row['sample']}\\\"\")\n",
    "\n",
    "# create a more structured table view\n",
    "html_table = \"<table><tr><th>Model Type</th><th>Configuration</th><th>Generated Text</th></tr>\"\n",
    "for _, row in samples_df.iterrows():\n",
    "    html_table += f\"<tr><td>{row['type']}</td><td>{row['config']}</td><td>{row['sample']}</td></tr>\"\n",
    "html_table += \"</table>\"\n",
    "\n",
    "from IPython.display import HTML\n",
    "display(HTML(html_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7c1124",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This notebook has demonstrated:\n",
    "\n",
    "- A comprehensive comparison between classical N-gram models, neural N-gram models, and Mini-GPT architectures\n",
    "- How tokenizer choices significantly impact language model performance, with larger vocabulary sizes not always producing better results\n",
    "- The trade-off between model complexity and performance across different architectures\n",
    "\n",
    "## Key observations:\n",
    "\n",
    "- Classical N-gram models remain surprisingly effective for simple language tasks, especially with advanced smoothing techniques\n",
    "- Neural models achieve lower perplexity but require significantly more training time compared to classical approaches\n",
    "- Tokenizer choice is important\n",
    "- Text generation quality doesn't always correlate with perplexity metrics, highlighting the importance of qualitative evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a36e02b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
